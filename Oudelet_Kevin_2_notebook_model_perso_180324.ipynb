{"cells":[{"attachments":{},"cell_type":"markdown","id":"c10447d1","metadata":{"id":"c10447d1"},"source":["## **Classez des images**\n","\n","### partie 2/4 : modèle perso\n","\n","<br>\n","\n","> #### notebook de mise en oeuvre de création et d’entraînement du modèle personnel, des simulations des différentes valeurs des hyperparamètres et de data augmentation.. <br><br>\n","\n","<br>\n"]},{"cell_type":"markdown","metadata":{},"source":["## 0 Imports\n"]},{"attachments":{},"cell_type":"markdown","id":"8cf10133","metadata":{"id":"8cf10133"},"source":["### 0.1 Librairies, réglages\n"]},{"cell_type":"code","execution_count":null,"id":"6ffe8b0d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9566,"status":"ok","timestamp":1688504146442,"user":{"displayName":"Kevin Oudelet","userId":"05301463766297982835"},"user_tz":-120},"id":"6ffe8b0d","outputId":"4abc7d5f-4fe6-46db-fa59-05891e583e93"},"outputs":[],"source":["# paths, folders/files\n","import os, sys, random, re\n","from os import listdir\n","from glob import glob\n","from zipfile import ZipFile\n","import time\n","\n","# math, dataframes\n","import numpy as np\n","import pandas as pd\n","from pandarallel import pandarallel\n","from collections import Counter\n","\n","# Visualisation\n","from pprint import pprint\n","import matplotlib.pyplot as plt\n","from matplotlib.image import imread\n","import seaborn as sns\n","import plotly.express as px\n","# from wordcloud import WordCloud\n","# from PIL import Image\n","from optuna.visualization import plot_optimization_history\n","\n","# Feature engineering\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn import preprocessing\n","from sklearn import manifold, decomposition\n","from sklearn import cluster, metrics\n","from sklearn.model_selection import train_test_split\n","# from sklearn.feature_extraction.text import CountVectorizer\n","\n","# NN\n","import tensorflow as tf\n","from tensorflow.keras.metrics import Accuracy, Precision, Recall, AUC\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","\n","import keras_tuner as kt\n","import optuna\n","%pip install optuna-integration\n","from optuna_integration import TFKerasPruningCallback\n","\n","# tracking\n","from mlflow import MlflowClient\n","import mlflow.keras\n","\n","# ! REQUIRES CONSOLE COMMAND : mlflow ui\n","# depuis dossier notebooks\n","os.environ['MLFLOW_TRACKING_URI'] = './'\n","# Utilisable seulement en local ?\n","mlflow.set_tracking_uri(\"http://localhost:5000\")\n","client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")\n","\n","import pickle\n","\n","from tensorflow.keras import datasets, layers, models\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalAveragePooling1D, Flatten, Dense, Dropout\n","from tensorflow.keras.layers import Rescaling, RandomFlip, RandomRotation, RandomZoom\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","print('\\nPython version ' + sys.version)\n","print('Tensorflow version ' + tf.__version__)\n","print('Mlflow Autolog compatible with: 2.2.4 <= keras <= 2.6.0')\n","print('Keras version ' + tf.keras.__version__)\n","# no autolog... ?\n","\n","# plt.style.use('ggplot')\n","pd.set_option('display.max_columns', 200)\n","\n","# Modify if necessary\n","num_cores = os.cpu_count()\n","print(f\"\\nNumber of CPU cores: {num_cores}\")\n","pandarallel.initialize(progress_bar=False, nb_workers=6)\n"]},{"cell_type":"markdown","id":"6dc1e6dd","metadata":{},"source":["### 0.2 Variables globales\n"]},{"cell_type":"code","execution_count":null,"id":"5e17172c","metadata":{},"outputs":[],"source":["nb_classes = 10      # min 2, max 120 (theorique, en pratique : 3, 4 ,5 , 10).\n","                    # possible creer + en utilisant le ntbk 1\n","\n","size_wh = 128\n","target_size=(size_wh, size_wh) # pour grille 5x5, stride (2,2) ?\n","\n","test_size = 0.15 # same for validation\n","\n","alea = 42 # pour fixer les ttsplits et tjs travailler sur les mm datasets\n","# En revanche l'initialisation des poids des modèles restera aléatoire,\n","# pour pouvoir comparer les resultats sur +ieurs runs.\n","\n","epochs = 15\n","\n","nb_runs = 20 # 2-3 le matin, 50 le soir\n","\n","pickle_path_model = f'./pickle/best_{nb_classes}/ze_model.pkl'\n","pickle_path_accuracy = f'./pickle/best_{nb_classes}/best_accuracy_val.pkl'\n","pickle_path_params = f'./pickle/best_{nb_classes}/best_params.pkl'\n","\n","try:\n","    best_acc = pickle.load(open(pickle_path_accuracy, 'rb'))\n","    best_parameters = pickle.load(open(pickle_path_params, 'rb'))\n","except:\n","    best_acc = 0.3\n","\n","print(best_acc, '\\n')\n","pprint(best_parameters)\n"]},{"cell_type":"markdown","id":"93756304","metadata":{},"source":["### 0.3 Fonctions\n"]},{"cell_type":"code","execution_count":null,"id":"93319b21","metadata":{},"outputs":[],"source":["# tracking manuel\n","\n","model_results = []\n","\n","def affichage_results(multiple=True, best_acc=best_acc):\n","    \"\"\"Tracking manuel de nos modèles (data, params, tps, scores) pour comparaison.\"\"\"\n","\n","    print(f'{nb_classes} classes prédites')\n","    print(f'dim images : {size_wh} x {size_wh} x 3')\n","    print(f'size validation & testing sets : {test_size}')\n","    print(f'nb epochs : {epochs}', '\\n')\n","    print(f'current best accuracy (validation set) : {best_acc}')\n","\n","    if multiple:\n","        print(f'nb runs: {nb_runs}', '\\n')\n","\n","    # Create a DataFrame from the list of model results\n","    model_comparison_df = pd.concat([pd.DataFrame(model_results)], ignore_index=True)\n","\n","    # Sort the DataFrame by precision in descending order (higher is better)\n","    model_comparison_df.sort_values(by='accuracy_val_moy', ascending=False, inplace=True)\n","\n","    # Display the sorted DataFrame\n","    display(model_comparison_df)\n","\n","\n","# tracking mlflow\n","\n","def create_mlflow_experiment(\n","    experiment_name: str, artifact_location: str, tags: dict[str, str]\n",") -> str:\n","    \"\"\"\n","    Create a new mlflow experiment with the given name and artifact location.\n","\n","    Parameters:\n","    ----------\n","    experiment_name: str\n","        The name of the experiment to create.\n","    artifact_location: str\n","        The artifact location of the experiment to create.\n","    tags: dict[str,Any]\n","        The tags of the experiment to create.\n","\n","    Returns:\n","    -------\n","    experiment_id: str\n","        The id of the created experiment.\n","    \"\"\"\n","    try:\n","        experiment_id = mlflow.create_experiment(\n","            name=experiment_name, artifact_location=artifact_location, tags=tags\n","        )\n","    except:\n","        print(f\"Experiment {experiment_name} already exists.\")\n","        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n","\n","    mlflow.set_experiment(experiment_name=experiment_name)\n","\n","    return experiment_id\n","\n","\n","def get_mlflow_experiment(\n","    experiment_id: str = None, experiment_name: str = None\n",") -> mlflow.entities.Experiment:\n","    \"\"\"\n","    Retrieve the mlflow experiment with the given id or name.\n","\n","    Parameters:\n","    ----------\n","    experiment_id: str\n","        The id of the experiment to retrieve.\n","    experiment_name: str\n","        The name of the experiment to retrieve.\n","\n","    Returns:\n","    -------\n","    experiment: mlflow.entities.Experiment\n","        The mlflow experiment with the given id or name.\n","    \"\"\"\n","    if experiment_id is not None:\n","        experiment = mlflow.get_experiment(experiment_id)\n","    elif experiment_name is not None:\n","        experiment = mlflow.get_experiment_by_name(experiment_name)\n","    else:\n","        raise ValueError(\"Either experiment_id or experiment_name must be provided.\")\n","\n","    return experiment\n"]},{"cell_type":"markdown","id":"00274e86","metadata":{},"source":["### 0.4 Metriques\n"]},{"cell_type":"code","execution_count":null,"id":"66c82819","metadata":{},"outputs":[],"source":["# J'utiliserai tjs les noms anglais des métriques ici,\n","# pour éviter la confusion précision (fr) != precision (en),\n","# et pour simplement garder les noms des fonctions importées depuis tf.keras.metrics\n","\n","# Nous sommes dans un cas de classification \"classique\", 1 classe prédite.\n","# Une première métrique simple et intuitive est donc l'accuracy :\n","# nb de prédictions correctes / nb total de prédictions.\n","# Cette métrique nous suffit déjà pour comparer et optimiser nos modèles.\n","\n","# Si l'on souhaite étudier + en détail les prédictions des modèles, on utilisera\n","# la precison et le recall\n","\n","# Precison (TP / (TP + FP)) :\n","# Une précision élevée signifie que si une classe est prédite par le modèle,\n","# alors il y a une forte probabilité (égale à la precision)\n","# que le chien appartienne en effet à cette classe.\n","\n","# Recall (TP / (TP + FN)) :\n","# pour évaluer la capacité des modèles à identifier toutes les instances positives.\n","# Exemple : Si notre precision est égale à 1, c'est parfais, cela signifie que\n","# pour une une classe au moins, toutes les valeurs prédites par le modèle sont correctes.\n","# Cependant, il est possible que cela ne concerne que très peu de cas (mettons, 1 ou 2 prédictions)\n","# et qu'à côté de cela le modèle a pu faire des milliers de prédictions incorrectes,\n","# la precision seule ne nous le dit pas.\n","\n","# Comme l'amélioration de la precision se fait svt au détriment du recall, en pratique\n","# on combine souvent les 2 avec le f1score (= moyenne harmonique)\n","# (2 x precision x recall) / (precision + recall)\n","# qui nous donne directement une idée du compromis precision / recall\n","\n","# petit souci : le f1score et keras, c'est tout une histoire...\n","# Dans les versions récentes (depuis la 2.15.0 il me semble), le f1score est directement intégré\n","# au module metrics. Le problème est que conda n'arrive pas à résoudre un env avec ces versions,\n","# incompatibles avec les requirements d'autres packages dans l'env.\n","\n","# Dans les versions + anciennes de keras, le f1score était dans un autre module, \"addons\",\n","# mais ce moule est désormais déprécié.\n","# Solutions : définir un f1score custom\n","# Pas évident car tensorflow est très pointilleux sur les types d'objets qu'il accepte.\n","# Exemple : cette fonction est construite en utilisant 2 fonctions tf.keras,\n","# pourtant elle renvoit une erreur si on tente de l'utiliser parce que le type retourné est\n","# un float.\n","\n","def f1score(y_true, y_pred):\n","    precision = Precision(y_true, y_pred)\n","    recall = Recall(y_true, y_pred)\n","    f1 = (2*precision*recall) / (precision + recall)\n","\n","    return f1\n","\n","# Autre solution, utiliser le f1score de sk-learn. Compatibilité logging mlflow ?\n","\n","metrics=[\n","    'Accuracy',\n","    # f1score,\n","    # AUC(),\n","]\n"]},{"cell_type":"markdown","metadata":{},"source":["### 0.4 Data\n"]},{"cell_type":"code","execution_count":null,"id":"ed6a46d8","metadata":{},"outputs":[],"source":["data = pd.read_csv(f'./data/data_{nb_classes}_classes.csv', sep=',')\n","\n","print(data.shape)\n","data.head()\n"]},{"attachments":{},"cell_type":"markdown","id":"a334fc5e","metadata":{"id":"a334fc5e"},"source":["### 0.5 Etude de faisabilité (sort of)\n"]},{"cell_type":"code","execution_count":null,"id":"00ea9423","metadata":{},"outputs":[],"source":["# Ici l'étude de faisabilité préconisée par la méthode Agile n'est pas vraiment utile en tant que telle\n","# (On sait que le projet est faisable). Il s'agit plutôt de pouvoir observer le travail effectué par le\n","# bloc d'encodage, sans utiliser d'algorithme de prediction supervisée\n","# (algo classique ou plutôt, ici, bloc des layers fully connected)\n","\n","images_features = []\n","\n","for image_file in data[\"photo_path\"] :\n","    image = load_img(image_file, target_size=(180, 180))\n","    image = img_to_array(image)\n","    images_features.append(image)\n","\n","images_features = np.asarray(images_features)\n","images_features.shape\n"]},{"cell_type":"code","execution_count":null,"id":"7dafe1cd","metadata":{},"outputs":[],"source":["# Reshape images to flatten them into vectors\n","flattened_images = images_features.reshape(images_features.shape[0], -1)\n","print(flattened_images.shape)\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","normalized_images = scaler.fit_transform(flattened_images)\n"]},{"cell_type":"markdown","id":"e5331844","metadata":{},"source":["### 0.6 Réduction dim\n"]},{"cell_type":"code","execution_count":null,"id":"155e6954","metadata":{},"outputs":[],"source":["# PCA\n","\n","print(normalized_images.shape) # same as flattened_images\n","\n","pca = decomposition.PCA(n_components=0.99)\n","feat_pca= pca.fit_transform(normalized_images)\n","\n","print(feat_pca.shape)\n","\n","# dimention divisée par 200 (presque), en conservant 99% de la variance !\n"]},{"cell_type":"code","execution_count":null,"id":"1a7bed7c","metadata":{},"outputs":[],"source":["# Plot explained variance ratio\n","plt.figure(figsize=(8, 6))\n","plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--', color='#3af')\n","plt.xlabel('Number of Principal Components')\n","plt.ylabel('Cumulative Explained Variance Ratio')\n","plt.title('Cumulative Explained Variance Ratio vs. Number of Principal Components')\n","plt.grid(True)\n","plt.show()\n","\n","# Pourquoi on a besoin du tsne pour la visu : en 2D ou même en 3D, les 3 premiers vecteurs propres\n","# # fournis par la PCA ne captent \"que\" (environ) un tiers de l'information.\n","# Ce qu'on verrait serait très déformé par les projections successives de la PCA.\n","# tester ?\n"]},{"cell_type":"markdown","id":"1790650e","metadata":{},"source":["### 0.7 tsne\n"]},{"cell_type":"code","execution_count":null,"id":"01528ec8","metadata":{},"outputs":[],"source":["# t-sne\n","\n","tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000, init='random', random_state=6)\n","X_tsne = tsne.fit_transform(feat_pca)\n"]},{"cell_type":"code","execution_count":null,"id":"010e972c","metadata":{},"outputs":[],"source":["# encodage target\n","\n","label_encoder = preprocessing.LabelEncoder()\n","label_encoder.fit(data[\"breed\"])\n","\n","data[\"target\"] = label_encoder.transform(data[\"breed\"])\n","\n","display(data.head(1))\n","data.tail(1)\n"]},{"cell_type":"code","execution_count":null,"id":"e794502e","metadata":{},"outputs":[],"source":["df_tsne = pd.DataFrame(X_tsne, columns=['tsne1', 'tsne2'])\n","df_tsne[\"class\"] = data[\"target\"]\n","\n","plt.figure(figsize=(8,5))\n","sns.scatterplot(\n","    x=\"tsne1\", y=\"tsne2\",\n","    hue=\"class\",\n","    palette=sns.color_palette('tab10', n_colors=3), s=50, alpha=0.6,\n","    data=df_tsne,\n","    legend=\"brief\")\n","\n","plt.title('TSNE selon les vraies classes', fontsize = 30, pad = 35, fontweight = 'bold')\n","plt.xlabel('tsne1', fontsize = 26, fontweight = 'bold')\n","plt.ylabel('tsne2', fontsize = 26, fontweight = 'bold')\n","plt.legend(prop={'size': 14})\n","\n","plt.show()\n","\n","# Ca marche moins bien sans extraction de features !\n","# On retentera en fin de notebook, en utilisant notre modèle.\n"]},{"cell_type":"markdown","id":"5fbfb928","metadata":{},"source":["### 1 Création d'un premier modèle\n"]},{"cell_type":"code","execution_count":null,"id":"b0d17859","metadata":{},"outputs":[],"source":["# Notre objectifs principal ici est\n","# de pouvoir observer / comprendre la fonction des différentes layers utilisées.\n","\n","# Pour cela, nous allons commencer par une architecture très simple :\n","# le but n'est pas d'avoir le modèle le + performant possible.\n","# (irréaliste ici car on n'aurait ni le tps ni les ressources pour l'entrainer)\n","# (en revanche, voir le notebook 3, transfer learning, pour une comparaison de modèles + complexes)\n","\n","# Première idée :\n","# Notre modèle de base sera donc inspiré d'AlexNet, dont l'architecture est :\n","\n","# \"AlexNet contains eight layers: the first five are convolutional layers,\n","# some of them followed by max-pooling layers, and the last three are fully connected layers.\n","# [...] The entire structure can be written as:\n","\n","# (CNN -> RN -> MP)^2 -> (CNN^3 -> MP) -> (FC -> DO)^2 -> Linear -> softmax\n","\n","# where\n","# CNN = convolutional layer (with ReLU activation)\n","# RN = local response normalization\n","# MP = maxpooling\n","# FC = fully connected layer (with ReLU activation)\n","# Linear = fully connected layer (without activation)\n","# DO = dropout\n","\n","# It used the non-saturating ReLU activation function, which showed improved training performance\n","# over tanh and sigmoid.\" (wiki)\n"]},{"cell_type":"markdown","id":"dcaab114","metadata":{},"source":["### 1.1 LeNet inspired architecture\n"]},{"cell_type":"code","execution_count":null,"id":"c72ad8b8","metadata":{},"outputs":[],"source":["# Problème : 8 groupes de layers... (16 individuelles, en fait) C'est déjà beaucoup !\n","# On peut faire + simple, au moins pour commencer.\n","\n","# Voyons de quoi sera capable un modèle inspiré plutôt par LeNet-5\n","# et par ce notebook : https://www.kaggle.com/code/schmoyote/simple-cnn-architecture-for-image-classification/notebook\n","\n","\n","def model_v1():\n","    model = Sequential()\n","    model.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(size_wh, size_wh, 3)))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Conv2D(16, kernel_size=(5, 5), activation='tanh'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Flatten())\n","    # model.add(Dense(120, activation='tanh'))\n","    model.add(Dense(60, activation='tanh'))\n","    model.add(Dense(nb_classes, activation='softmax'))\n","\n","    return model\n","\n","\n","model = model_v1()\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n","\n","model.summary()\n"]},{"cell_type":"markdown","id":"91f12626","metadata":{},"source":["### 1.2 feature engineering\n"]},{"cell_type":"code","execution_count":null,"id":"1ebe7531","metadata":{},"outputs":[],"source":["feature = \"photo_path\"\n","\n","\n","def feature_to_array(feature=\"photo_path\", norm=False, shape_result=False):\n","    X_feature = []\n","\n","    for image_file in data[feature] :\n","        image = load_img(image_file, target_size=target_size)\n","        image = img_to_array(image)\n","        if norm:\n","            image = image / 255\n","        X_feature.append(image)\n","\n","    X_feature = np.asarray(X_feature)\n","\n","    if shape_result:\n","        pprint(X_feature[:1])\n","        print('\\n', \"Shape of X_train:\", X_feature.shape)\n","        # ok\n","\n","    return X_feature\n","\n","\n","X_feature = feature_to_array(shape_result=True)\n"]},{"cell_type":"markdown","id":"6a22a0d0","metadata":{},"source":["### 1.3 label encoding target\n"]},{"cell_type":"code","execution_count":null,"id":"1b8fa580","metadata":{},"outputs":[],"source":["y_target = np.asarray(data[\"target\"])\n","print(y_target.shape)\n","pprint(y_target)\n"]},{"cell_type":"markdown","id":"02f5b733","metadata":{},"source":["### 1.4 train test split\n"]},{"cell_type":"code","execution_count":null,"id":"560656c8","metadata":{},"outputs":[],"source":["X_train_val, X_test, y_train_val, y_test = train_test_split(X_feature, y_target, test_size=test_size,\n","                                                            shuffle=True, random_state=alea,\n","                                                            stratify=y_target) # important\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=test_size,\n","                                                            shuffle=True, random_state=alea,\n","                                                            stratify=y_train_val)\n","\n","print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape, '\\n')\n","\n","print(y_train.shape)\n","print(y_val.shape)\n","print(y_test.shape, '\\n')\n"]},{"cell_type":"markdown","id":"a1d114a8","metadata":{},"source":["### 1.5 one hot encoding (targets)\n"]},{"cell_type":"code","execution_count":null,"id":"1ee0d23f","metadata":{},"outputs":[],"source":["# One-hot encode target values after the split to avoid data leakage\n","\n","y_train_ohe = tf.keras.utils.to_categorical(y_train)\n","y_val_ohe = tf.keras.utils.to_categorical(y_val)\n","y_test_ohe = tf.keras.utils.to_categorical(y_test)\n"]},{"cell_type":"markdown","id":"b49e2120","metadata":{},"source":["### 1.6 Training\n"]},{"cell_type":"code","execution_count":null,"id":"05cbd2f5","metadata":{},"outputs":[],"source":["# Train the model\n","model.fit(X_train, y_train_ohe, epochs=epochs, batch_size=32,\n","          validation_data=(X_val, y_val_ohe), verbose=0) # verbose=0 avoids crashing cell later\n"]},{"cell_type":"markdown","id":"5f18e540","metadata":{},"source":["### 1.7 Evaluation\n"]},{"cell_type":"code","execution_count":null,"id":"ccf7cf32","metadata":{},"outputs":[],"source":["# On overfit dès le début ??\n","\n","# Evaluate the model\n","val_loss_ref, val_acc_ref = model.evaluate(X_val, y_val_ohe)\n","print(f'Val accuracy: {val_acc_ref}', '\\n')\n","\n","# En prédisant au hasard on aurait une chance sur 3, autrement dit\n","# ce modèle fait des prédictions quasi-aléatoires.\n","# Pas terrible, mais c'est un début !\n","\n","# avant, tester sur photos d'origine (pour évaluer l'utilité du prétraitement effectué)\n","\n"]},{"cell_type":"markdown","id":"35c901c0","metadata":{},"source":["### save\n"]},{"cell_type":"code","execution_count":null,"id":"a645cc2f","metadata":{},"outputs":[],"source":["parameters = {\n","    \"model\": \"V1\",\n","    \"preprocessing\": 'no_norm',\n","    \"feature\": feature,\n","    'target': f'{nb_classes}_classes',\n","    'dim images': f'{size_wh} x {size_wh} x 3',\n","    'size val test sets': test_size,\n","    'nb epochs': epochs\n","}\n","\n","\n","def save_model_if_better_than_the_best(model=model, acc=val_acc_ref, best_params=parameters):\n","\n","    global best_acc\n","\n","    if acc > best_acc:\n","        print(f'Improving! Accuracy up to {acc}')\n","        best_acc = acc\n","        # erreur logging, saving instead\n","        # using mlflow\n","        # mlflow.keras.save_model(model, \"./best_model\") # works only once\n","        # using pickle\n","        with open(pickle_path_model, 'wb') as f:\n","            pickle.dump(model, f)\n","        # also save best accuracy\n","        with open(pickle_path_accuracy, 'wb') as f:\n","            pickle.dump(best_acc, f)\n","        # and corresponding best params\n","        with open(pickle_path_params, 'wb') as f:\n","            pickle.dump(best_params, f)\n","\n","\n","save_model_if_better_than_the_best()\n"]},{"cell_type":"code","execution_count":null,"id":"c7e3160d","metadata":{},"outputs":[],"source":["# petit tracking manuel\n","results = {'model': 'V1',\n","            'df': 'data_3_classes',\n","            'feature': feature,\n","            'accuracy_val_moy': val_acc_ref,\n","            'time_fit': 'to do',\n","            'time_predict':'to do'\n","            }\n","\n","# Append a new row for this model\n","model_results.append(results)\n"]},{"cell_type":"markdown","id":"6d8286ee","metadata":{},"source":["### 1.8 test utilité prétraitements\n"]},{"cell_type":"code","execution_count":null,"id":"c88fbe03","metadata":{},"outputs":[],"source":["experiment_id = create_mlflow_experiment(\n","    experiment_name=\"analyse_pretraitements\",\n","    artifact_location=\"./mlruns/artifacts\",\n","    tags={\"model\": \"V1\", \"preprocessing\": 'no_norm', \"features\": \"7_processed_features\", 'target': '3_classes'},\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"b8b78821","metadata":{},"outputs":[],"source":["# resized\n","\n","\n","def test_feature(df=data, feature='photo_path', epochs=epochs):\n","    parameters = {\n","        \"model\": \"V1\",\n","        \"preprocessing\": 'no_norm',\n","        \"feature\": feature,\n","        'target': f'{nb_classes}_classes',\n","        'dim images': f'{size_wh} x {size_wh} x 3',\n","        'size val test sets': test_size,\n","        'nb epochs': epochs\n","    }\n","    mlflow.log_params(parameters)\n","\n","    # feature, target\n","    X_feature = feature_to_array(feature)\n","    y_target = np.asarray(df[\"target\"])\n","\n","    #tt split\n","    X_train_val, X_test, y_train_val, y_test = train_test_split(X_feature, y_target, test_size=0.1,\n","                                                                shuffle=True, random_state=alea,\n","                                                                stratify=y_target) # important\n","\n","    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1,\n","                                                                shuffle=True, random_state=alea,\n","                                                                stratify=y_train_val)\n","\n","    # ohe cible\n","    y_train_ohe = tf.keras.utils.to_categorical(y_train)\n","    y_val_ohe = tf.keras.utils.to_categorical(y_val)\n","    y_test_ohe = tf.keras.utils.to_categorical(y_test)\n","\n","    # model\n","    model = model_v1()\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n","\n","    model.fit(X_train, y_train_ohe, epochs=epochs, batch_size=32,\n","            validation_data=(X_val, y_val_ohe), verbose=0)\n","\n","    _, val_acc = model.evaluate(X_val, y_val_ohe, verbose=0)\n","\n","    save_model_if_better_than_the_best(model=model, acc=val_acc, best_params=parameters)\n","\n","    metrix = {\n","        \"accuracy_val\": val_acc\n","    }\n","    # multiple metrics\n","    mlflow.log_metrics(metrix)\n","\n","    print(f'Val accuracy (feature={feature}): {val_acc}')\n","\n","    results = {'model': 'V1',\n","            'df': 'data_3_classes',\n","            'feature': feature,\n","            'accuracy_val_moy': val_acc,\n","            'time_fit': 'to do',\n","            'time_predict':'to do'\n","            }\n","\n","    # Append a new row for this model\n","    model_results.append(results)\n","\n","    return val_acc\n","\n","\n","# _ = test_feature(df=data, feature='resized')\n","\n","\n","# 0.6 de precision sans pretraitement (parfois 0.3 ??), 0.3 avec.\n","# Notre prétraitement semble (très) contre-productif. Ajuster dim, filtres ? (trop flou ?)\n","# Comme prétraitement, les méthodes .preprocessing() de keras consistent svt seulement en\n","# redimensionnemt + normalisation.\n","# faire pareil ?\n"]},{"cell_type":"code","execution_count":null,"id":"20a2dc28","metadata":{},"outputs":[],"source":["# nested\n","\n","features_to_test = ['resized', 'expo', 'contraste', 'denoised_g', 'denoised_bi', 'denoised_nl',\n","                    'preprocess_complet']\n","\n","\n","experiment = get_mlflow_experiment(experiment_id=experiment_id)\n","print(\"Name: {}\".format(experiment.name))\n","\n","with mlflow.start_run(run_name=\"7_processed_features_once\", experiment_id=experiment_id) as parent:\n","    all_metrics = {}\n","\n","    for feature in features_to_test:\n","        print('\\n', feature)\n","        with mlflow.start_run(run_name=f'{feature}', nested=True) as child:\n","            print(f'RUN ID child_{feature}: {child.info.run_id}', '\\n')\n","            accuracy_val = test_feature(df=data, feature=feature)\n","            all_metrics[f'accuracy_{feature}'] = accuracy_val\n","\n","    print('\\n')\n","    pprint(all_metrics)\n","    mlflow.log_metrics(all_metrics)\n","\n","# results\n","# 'resized' 0.32 ???\n","# 'expo' 0.63\n","# 'contraste' 0.32\n","\n","# rappel\n","print(f'\\nRappel : Val accuracy (photo_path original): {val_acc_ref}')\n"]},{"cell_type":"markdown","id":"6e3ee68f","metadata":{},"source":["### Comparaison\n"]},{"cell_type":"code","execution_count":null,"id":"9d78a24b","metadata":{},"outputs":[],"source":["affichage_results(multiple=False, best_acc=best_acc)\n","\n","# On y voit déjà (un peu) + clair :\n","# Chaque étape de notre prétraitement semble + ou - détériorer la qualité des prédictions.\n","\n","# Encore que... ?\n","# ??? resized et photo_path devraient donner des résultats bien + proches, non ??\n","# sets identiques sauf dim, et redim lors de création de X_feature\n","# ... devraient être exactement identiques, et donc avoir des resultats proches\n","\n","# En fait d'un run à l'autre, les prédictions varient énormément...\n","# difficile du coup d'évaluer l'impact de nos prétraitements.\n","# moyenne sur +ieurs runs ?\n","\n","model_results = []\n","\n","# (Très) svt le modèle ne parvient pas à \"train ses layers\", et l'accuracy des prédictions\n","# sur le jeu de validation reste au niveau de prédictions random.\n"]},{"cell_type":"markdown","id":"ca35c058","metadata":{},"source":["### Mean multiple runs\n"]},{"cell_type":"code","execution_count":null,"id":"e6823f33","metadata":{},"outputs":[],"source":["experiment_id = create_mlflow_experiment(\n","    experiment_name='analyse_pretraitmt_multiple_runs',\n","    artifact_location=\"./mlruns/artifacts\",\n","    tags={\"model\": \"V1\", \"preprocessing\": 'no_norm', \"features\": \"all_8_features\", 'target': '3_classes'},\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"69a97743","metadata":{},"outputs":[],"source":["#\n","\n","\n","def test_feature_n_times(feature='photo_path', epochs=epochs, n=nb_runs):\n","    \"\"\"\n","\n","    \"\"\"\n","    parameters = {\n","        \"model\": \"V1\",\n","        \"preprocessing\": 'no_norm',\n","        \"feature\": feature,\n","        'target': f'{nb_classes}_classes',\n","        'dim images': f'{size_wh} x {size_wh} x 3',\n","        'size val test sets': test_size,\n","        'nb epochs': epochs\n","    }\n","    mlflow.log_params(parameters) # child\n","\n","    results_val_acc, results_time_fit,  results_time_predict = [], [], []\n","\n","    for i in range(n):\n","\n","        with mlflow.start_run(run_name=f'run_{i}', nested=True) as grand_kid:\n","            print(f'\\nRun {i}\\n')\n","            print(\"RUN ID \", grand_kid.info.run_id )\n","            mlflow.log_params(parameters) # grand_kid\n","\n","            # feature, target\n","            X_feature = feature_to_array(feature)\n","            y_target = np.asarray(data[\"target\"])\n","\n","            X_train_val, X_test, y_train_val, y_test = train_test_split(X_feature, y_target, test_size=0.1,\n","                                                                        shuffle=True, random_state=alea,\n","                                                                        stratify=y_target) # important\n","\n","            X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1,\n","                                                                        shuffle=True, random_state=alea,\n","                                                                        stratify=y_train_val)\n","\n","            y_train_ohe = tf.keras.utils.to_categorical(y_train)\n","            y_val_ohe = tf.keras.utils.to_categorical(y_val)\n","            y_test_ohe = tf.keras.utils.to_categorical(y_test)\n","\n","            # model\n","            model = model_v1()\n","\n","            # Compile the model\n","            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","            # fit model and time it\n","            time_fit_start = time.time() # returns the time in seconds since the epoch.\n","            model.fit(X_train, y_train_ohe, epochs=epochs, batch_size=32,\n","                    validation_data=(X_val, y_val_ohe), verbose=0)\n","            time_fit_end = time.time()\n","            time_fit = time_fit_end - time_fit_start\n","\n","            # time predictions\n","            time_predict_start = time.time()\n","            _, val_acc = model.evaluate(X_val, y_val_ohe) # , verbose=0 ?\n","            time_predict_end = time.time()\n","            time_predict = time_predict_end - time_predict_start\n","\n","            # print(f'Val accuracy (feature={feature}): {val_acc}')\n","\n","            save_model_if_better_than_the_best(model=model, acc=val_acc, best_params=parameters)\n","\n","            metriq = {\n","                \"accuracy_val\": val_acc,\n","                \"time_fit\": time_fit,\n","                \"time_predict\": time_predict\n","            }\n","            mlflow.log_metrics(metriq)\n","\n","            results_val_acc.append(val_acc)\n","            results_time_fit.append(time_fit)\n","            results_time_predict.append(time_predict)\n","\n","            # Log the model doesn't work ??\n","            # mlflow.keras.log_model(model=model, artifact_path=\"good_model\",registered_model_name='best_model')\n","\n","    # moyennes\n","    mean_val_acc = np.mean(results_val_acc)\n","    mean_time_fit = np.mean(results_time_fit)\n","    mean_time_predict = np.mean(results_time_predict)\n","\n","    # écarts-types (utile ici pour le score, afin d'avoir une idée de la \"régularité\" des résultats)\n","    # les tps d'entrainement / prédiction st bcp + stables\n","    std_val_acc = np.std(results_val_acc)\n","\n","    metrix = {\n","        \"accuracy_val_moy\": mean_val_acc,\n","        \"accuracy_val_std\": std_val_acc,\n","        \"time_fit_moy\": mean_time_fit,\n","        \"time_predict_moy\": mean_time_predict\n","    }\n","    for run in range(nb_runs):\n","        metrix[f'accuracy_val_run_{run}'] = results_val_acc[run]\n","\n","    # multiple metrics\n","    mlflow.log_metrics(metrix)\n","\n","    results = {'model': 'V1',\n","        'df': 'data_3_classes',\n","        'feature': feature,\n","        'accuracy_val_moy': mean_val_acc,\n","        'accuracy_val_std': std_val_acc,\n","        'time_fit_moy (s)': mean_time_fit,\n","        'time_predict_moy (s)': mean_time_predict,\n","        }\n","\n","    # Append a new row for this model\n","    model_results.append(results)\n","\n","    print('\\n', f'Accuracy val {feature}, moyenne : {mean_val_acc}')\n","    print(f'Accuracy val {feature}, écart-type : {std_val_acc}')\n","\n","    # + tard\n","    # log model\n","    # mlflow.sklearn.log_model(sk_model=knn_ref, artifact_path=\"special_knn\")\n","\n","     # Save a plot\n","    # plt.savefig(\"./artifacts/jaccard_plot_3_4_tags.png\")\n","    # Log the saved figure using MLflow\n","    # mlflow.log_artifact(\"./artifacts/jaccard_plot_3_4_tags.png\")\n","\n","    return metrix\n","\n","\n","# _ = test_feature_n_times()\n","\n","#\n"]},{"cell_type":"code","execution_count":null,"id":"517e92cc","metadata":{},"outputs":[],"source":["# nested\n","\n","all_features_to_test = ['photo_path'] + features_to_test\n","\n","\n","experiment = get_mlflow_experiment(experiment_id=experiment_id)\n","print(\"Name: {}\".format(experiment.name))\n","\n","\n","with mlflow.start_run(run_name=\"test_all_8_features\", experiment_id=experiment_id) as parent:\n","    parameters = {\n","        \"model\": \"V1\",\n","        \"preprocessing\": 'no_norm',\n","        \"feature\": feature,\n","        'target': f'{nb_classes}_classes',\n","        'dim images': f'{size_wh} x {size_wh} x 3',\n","        'size val test sets': test_size,\n","        'nb epochs': epochs\n","    }\n","    mlflow.log_params(parameters) # parent\n","\n","    all_metrics = {}\n","\n","    for feature in all_features_to_test:\n","        print('\\n', feature)\n","        with mlflow.start_run(run_name=f'{feature}', nested=True) as child:\n","            print(f'RUN ID child_{feature}: {child.info.run_id}', '\\n')\n","            metrix = test_feature_n_times(feature=feature)\n","            all_metrics[f'accuracy_val_moy_{feature}'] = metrix['accuracy_val_moy']\n","\n","    print('\\n')\n","    pprint(all_metrics)\n","    mlflow.log_metrics(all_metrics) # perfect\n","\n","\n","# results\n","# 'resized' 0.32 ???\n","# 'expo' 0.63\n","# 'contraste' 0.32\n"]},{"cell_type":"code","execution_count":null,"id":"accc094c","metadata":{},"outputs":[],"source":["affichage_results(best_acc=best_acc)\n","\n","# Enfin des résultats interprétables !\n","# Cpdt le nb de runs est encore très faible, et les résultats st très proches.\n","\n","# model_results = []\n","\n","# log / picle model. size ?\n","\n","# les pretraitements expo / contraste semblent bénéfiques,\n","# en revanche le filtre gaussien mm léger ici semble avoir un impact légèrement négatif\n","# sur les prédictions du modèle. Le modèle n'aime pas le flou !\n","\n","# (encore une fois, ces \"conclusions\" sont à prendre \"avec des pincettes\", en attendant\n","# des runs supplémentaires.)\n"]},{"cell_type":"code","execution_count":null,"id":"bede2b41","metadata":{},"outputs":[],"source":["# A propos de la fonction d'activation tanh\n","\n","# Choisi tanh pour commencer, une fonction d'activation \"historique\",\n","# +tôt que relu (+ utilisée dans les archis des modèles + récents)\n","# parce que la tangeante hyperbolique agit comme un scaler :\n","# Elle centre les valeurs autour de 0, sur l'intervalle [-1, 1]\n","\n","# du coup pas besoin de normaliser nos pixels au préalable.\n","# Quand on testera relu, il faudra comparer avec ou sans normalisation.\n","\n","# Autre caractéristique intéressante de la fonction tanh :\n","\n","# La symétrie par rapport à l'origine + normalisation peuvent aider les algorithmes à optimiser le modèle,\n","# en empêchant les gradients de devenir trop proches de 0 (problème fréquent du 'vanishing gradient')\n","# ou au contraire trop importants ('exploding gradient') pdt la backpropagation.\n","# Il est cpdt tjs possible de rencontrer un vanishing gradient.\n"]},{"cell_type":"markdown","id":"fe001dc4","metadata":{},"source":["## 2 Ameliorations\n"]},{"cell_type":"code","execution_count":null,"id":"87f62f99","metadata":{},"outputs":[],"source":["# 3 pistes -> améliorations possibles :\n","\n","# retour au preprocessing,\n","# data augmentation,\n","# améliorer le model lui-même.\n","\n","# Nous allons commencer par le modèle, car parvenir à obtenir un modèle + consistent\n","# (= faire baisser la sdt) rendrait l'analyse des pretraitements et de la data augmentation\n","# bcp + fiable.\n"]},{"cell_type":"markdown","id":"5845bbef","metadata":{},"source":["### keras-tuner, archi 1\n"]},{"cell_type":"code","execution_count":null,"id":"7216724f","metadata":{},"outputs":[],"source":["experiment_id = create_mlflow_experiment(\n","    experiment_name='keras_tuner_1_archi',\n","    artifact_location=\"./mlruns/artifacts\",\n","    tags={\"model\": \"V2\", \"preprocessing\": 'no_norm', \"feature\": \"photo_path\", 'target': '3_classes'},\n",")\n","\n","experiment = get_mlflow_experiment(experiment_id=experiment_id)\n","print(\"Name: {}\".format(experiment.name))\n"]},{"cell_type":"code","execution_count":null,"id":"25b3ac0e","metadata":{},"outputs":[],"source":["# tester Optuna ? (compatible sk-learn + tracking ui)\n","# kt très facile à intégrer ici, puisque nous utilisons uniquement keras\n","# syntaxe légèrement + simple (mais les 2 st assez proches)\n","# (sauf / f1score ?)\n","\n","# Commençons par qq hyperparams de l'architecture générale de notre modèle :\n","# (On espère \"alléger\" le modèle, pour le rendre plus facile a entrainer.)\n","# nb de filtres (pour les 2 couches de convolution)\n","# nb neurons (first fully-connected layer)\n","\n","\n","def build_model(hp):\n","\n","    model = Sequential()\n","    model.add(Conv2D(hp.Int(\"conv_1_units\", min_value=4, max_value=32, step=4),\n","                     kernel_size=(5, 5), activation='tanh', input_shape=(size_wh, size_wh, 3)))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Conv2D(hp.Int(\"conv_2_units\", min_value=4, max_value=32, step=4),\n","                     kernel_size=(5, 5), activation='tanh'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(hp.Int(\"dense_units\", min_value=32, max_value=192, step=32), activation='tanh'))\n","    model.add(Dense(nb_classes, activation='softmax'))\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","\n","def test_archi_n_times(feature='photo_path', epochs=epochs, n=nb_runs, model_name='V2_kt',\n","                       norm=False, dir='keras_tuner/archi_1'):\n","    \"\"\"\n","    \"\"\"\n","\n","    tuner = kt.Hyperband(build_model,\n","                         objective='val_accuracy',\n","                         max_epochs=epochs,\n","                         factor=3,\n","                         directory=dir,\n","                         project_name=model_name)\n","\n","    results_val_acc, results_time_fit,  results_time_predict = [], [], []\n","\n","    for i in range(n):\n","        with mlflow.start_run(run_name=f'run_{i}', nested=True) as kid:\n","            print(f'\\nRun {i}\\n')\n","            print(\"RUN ID \", kid.info.run_id)\n","\n","            # feature, target\n","            X_feature = feature_to_array(feature, norm=norm)\n","            y_target = np.asarray(data[\"target\"])\n","\n","            X_train_val, X_test, y_train_val, y_test = train_test_split(X_feature, y_target, test_size=0.1,\n","                                                                        shuffle=True, random_state=alea,\n","                                                                        stratify=y_target) # important\n","\n","            X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1,\n","                                                                        shuffle=True, random_state=alea,\n","                                                                        stratify=y_train_val)\n","\n","            y_train_ohe = tf.keras.utils.to_categorical(y_train)\n","            y_val_ohe = tf.keras.utils.to_categorical(y_val)\n","            y_test_ohe = tf.keras.utils.to_categorical(y_test)\n","\n","            tuner.search(X_train, y_train_ohe, validation_data=(X_val, y_val_ohe))\n","\n","            # Get the optimal hyperparameters\n","            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","            # Build the model with the optimal hyperparameters\n","            model = tuner.hypermodel.build(best_hps)\n","\n","            # Fit model and time it\n","            time_fit_start = time.time()\n","            model.fit(X_train, y_train_ohe, epochs=epochs, batch_size=32,\n","                    validation_data=(X_val, y_val_ohe), verbose=0)\n","            time_fit_end = time.time()\n","            time_fit = time_fit_end - time_fit_start\n","\n","            # Time predictions\n","            time_predict_start = time.time()\n","            _, val_acc = model.evaluate(X_val, y_val_ohe) # , verbose=0 ?\n","            time_predict_end = time.time()\n","            time_predict = time_predict_end - time_predict_start\n","\n","            # print(f'Val accuracy (feature={feature}): {val_acc}', '\\n')\n","\n","            parameters.update(best_hps.values)\n","            mlflow.log_params(parameters)\n","\n","            save_model_if_better_than_the_best(model=model, acc=val_acc, best_params=parameters)\n","\n","            metriq = {\n","                \"accuracy_val\": val_acc,\n","                \"time_fit\": time_fit,\n","                \"time_predict\": time_predict\n","            }\n","            mlflow.log_metrics(metriq)\n","\n","\n","            results_val_acc.append(val_acc)\n","            results_time_fit.append(time_fit)\n","            results_time_predict.append(time_predict)\n","\n","    # Calculate averages\n","    mean_val_acc = np.mean(results_val_acc)\n","    mean_time_fit = np.mean(results_time_fit)\n","    mean_time_predict = np.mean(results_time_predict)\n","\n","    # Calculate standard deviations\n","    std_val_acc = np.std(results_val_acc)\n","\n","    metrix = {\n","        \"accuracy_val_moy\": mean_val_acc,\n","        \"accuracy_val_std\": std_val_acc,\n","        \"time_fit_moy\": mean_time_fit,\n","        \"time_predict_moy\": mean_time_predict\n","    }\n","    for run in range(nb_runs):\n","        metrix[f'accuracy_val_run_{run}'] = results_val_acc[run]\n","\n","    # multiple metrics\n","    mlflow.log_metrics(metrix)\n","\n","    results = {'model': model_name,\n","               'df': 'data_3_classes',\n","               'feature': feature,\n","               'accuracy_val_moy': mean_val_acc,\n","               'accuracy_val_std': std_val_acc,\n","               'time_fit_moy (s)': mean_time_fit,\n","               'time_predict_moy (s)': mean_time_predict,\n","              }\n","\n","    # Append a new row for this model\n","    model_results.append(results)\n","\n","    print(f'Accuracy val {feature}, moyenne : {mean_val_acc}')\n","    print(f'Accuracy val {feature}, écart-type : {std_val_acc}', '\\n')\n","\n","    print(\"Best Hyperparameters:\")\n","    print(best_hps.values)\n","\n","    return metrix\n","\n","\n","#  _ = test_archi_n_times()\n","\n","# wow\n","# Reloading Tuner from keras_tuner_dir/test_kt/tuner0.json\n","# kt ne perd pas de tps et récupère directement les hyperparamètres\n","# Au final il semble légèrement + rapide d'utiliser kt !\n"]},{"cell_type":"code","execution_count":null,"id":"6bf14312","metadata":{},"outputs":[],"source":["with mlflow.start_run(run_name=\"test_archi_1\", experiment_id=experiment_id) as parent:\n","    parameters = {\n","        \"model\": \"V2\",\n","        \"preprocessing\": 'no_norm',\n","        \"feature\": 'originale, photo_path',\n","        'target': f'{nb_classes}_classes',\n","        'dim images': f'{size_wh} x {size_wh} x 3',\n","        'size val test sets': test_size,\n","        'nb epochs': epochs\n","    }\n","    mlflow.log_params(parameters)\n","\n","    all_metrics = {}\n","\n","    metrix = test_archi_n_times()\n","\n","    all_metrics[f'accuracy_val_moy_{feature}'] = metrix['accuracy_val_moy']\n","\n","    print('\\n')\n","    pprint(all_metrics)\n","    mlflow.log_metrics(all_metrics)\n"]},{"cell_type":"code","execution_count":null,"id":"bf4265ba","metadata":{},"outputs":[],"source":["affichage_results(best_acc=best_acc)\n","\n","# Enfin un écart significatif !\n","# std tjs élevé (tjs l'aléatoire / initialisation)\n"]},{"cell_type":"markdown","id":"9c228fa6","metadata":{},"source":["### keras-tuner, fonction d'activation\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["experiment_id = create_mlflow_experiment(\n","    experiment_name='keras_tuner_2_activation',\n","    artifact_location=\"./mlruns/artifacts\",\n","    tags={\"model\": \"V3\", \"preprocessing\": 'no_norm', \"feature\": \"photo_path\", 'target': '3_classes'},\n",")\n","\n","experiment = get_mlflow_experiment(experiment_id=experiment_id)\n","print(\"Name: {}\".format(experiment.name))\n"]},{"cell_type":"code","execution_count":null,"id":"08d0cd74","metadata":{},"outputs":[],"source":["# Continuons en testant la fonction d'activation relu\n","\n","# tester Optuna ? (compatible sk-learn)\n","# kt très facile à intégrer ici, puisque nous utilisons uniquement keras\n","# (sauf / f1score ?)\n","\n","# Commençons par qq hyperparams de l'architecture générale de notre modèle :\n","# nb de filtres (pour les 2 couches de convolution)\n","# nb neurons (first fully-connected layer)\n","\n","def build_model(hp):\n","\n","    model = Sequential()\n","    model.add(Conv2D(hp.Int(\"conv_1_units\", min_value=4, max_value=32, step=4),kernel_size=(5, 5),\n","                     activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n","                     input_shape=(size_wh, size_wh, 3)))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Conv2D(hp.Int(\"conv_2_units\", min_value=4, max_value=32, step=4),\n","                     kernel_size=(5, 5), activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","    model.add(Flatten())\n","\n","    model.add(Dense(hp.Int(\"dense_units\", min_value=32, max_value=192, step=32),\n","                    activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])))\n","    model.add(Dense(nb_classes, activation='softmax'))\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","\n","with mlflow.start_run(run_name=\"test_activation\", experiment_id=experiment_id) as parent:\n","    parameters = {\n","        \"model\": \"V3\",\n","        \"preprocessing\": 'no_norm',\n","        \"feature\": 'originale, photo_path',\n","        'target': f'{nb_classes}_classes',\n","        'dim images': f'{size_wh} x {size_wh} x 3',\n","        'size val test sets': test_size,\n","        'nb epochs': epochs\n","    }\n","    mlflow.log_params(parameters)\n","\n","    all_metrics = {}\n","\n","    metrix = test_archi_n_times(model_name='V3_kt_activ', dir='keras_tuner/activ')\n","\n","    all_metrics[f'accuracy_val_moy_{feature}'] = metrix['accuracy_val_moy']\n","\n","    print('\\n')\n","    pprint(all_metrics)\n","    mlflow.log_metrics(all_metrics)\n","\n","\n","# {'conv_1_units': 28, 'activation': 'tanh', 'conv_2_units': 4, 'dense_units': 96\n"]},{"cell_type":"code","execution_count":null,"id":"cf78cbee","metadata":{},"outputs":[],"source":["affichage_results(best_acc=best_acc)\n","\n","# tjs très aléatoire.\n","# std ?\n","\n","# relu sans normalisation ici\n"]},{"cell_type":"markdown","id":"da45a255","metadata":{},"source":["### normalisation + fonction d'activation\n"]},{"cell_type":"code","execution_count":null,"id":"bd024189","metadata":{},"outputs":[],"source":["experiment_id = create_mlflow_experiment(\n","    experiment_name='keras_tuner_2_activation_norm',\n","    artifact_location=\"./mlruns/artifacts\",\n","    tags={\"model\": \"V3\", \"preprocessing\": 'normalisation', \"feature\": \"photo_path\", 'target': '3_classes'},\n",")\n","\n","experiment = get_mlflow_experiment(experiment_id=experiment_id)\n","print(\"Name: {}\".format(experiment.name))\n"]},{"cell_type":"code","execution_count":null,"id":"0a3bfde0","metadata":{},"outputs":[],"source":["with mlflow.start_run(run_name=\"activation_norm\", experiment_id=experiment_id) as parent:\n","    parameters = {\n","        \"model\": \"V3\",\n","        \"preprocessing\": 'norm',\n","        \"feature\": 'originale, photo_path',\n","        'target': f'{nb_classes}_classes',\n","        'dim images': f'{size_wh} x {size_wh} x 3',\n","        'size val test sets': test_size,\n","        'nb epochs': epochs\n","    }\n","    mlflow.log_params(parameters)\n","\n","    all_metrics = {}\n","\n","    metrix = test_archi_n_times(model_name='V3_kt_activ_norm', norm=True, dir='keras_tuner/norm_activ')\n","\n","    all_metrics[f'accuracy_val_moy_{feature}'] = metrix['accuracy_val_moy']\n","\n","    print('\\n')\n","    pprint(all_metrics)\n","    mlflow.log_metrics(all_metrics)\n"]},{"cell_type":"code","execution_count":null,"id":"bccf8d09","metadata":{},"outputs":[],"source":["affichage_results(best_acc=best_acc)\n"]},{"cell_type":"markdown","id":"06fbbff8","metadata":{},"source":["### optuna\n"]},{"cell_type":"code","execution_count":null,"id":"b3413dc5","metadata":{},"outputs":[],"source":["experiment_id = create_mlflow_experiment(\n","    experiment_name='optuna_1_activ',\n","    artifact_location=\"./mlruns/artifacts\",\n","    tags={\"model\": \"V4\", \"preprocessing\": 'no_norm', \"feature\": \"photo_path\", 'target': '3_classes'},\n",")\n","\n","experiment = get_mlflow_experiment(experiment_id=experiment_id)\n","print(\"Name: {}\".format(experiment.name))\n"]},{"cell_type":"code","execution_count":null,"id":"562b78ae","metadata":{},"outputs":[],"source":["# Contrairement à keras tuner, qui sauvegarde par défaut le résultat obtenu sur le disque,\n","# optuna refait les tests à chaque run\n","\n","\n","def build_model(conv_1_units, filter1, conv_2_units, filter2, fc_1_units):\n","    model = Sequential()\n","    model.add(Conv2D(conv_1_units, kernel_size=(filter1, filter1),\n","                     activation=\"tanh\",\n","                     input_shape=(size_wh, size_wh, 3)))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Conv2D(conv_2_units, kernel_size=(filter2, filter2),\n","                     activation=\"tanh\"))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(fc_1_units, activation='tanh'))\n","    model.add(Dense(nb_classes, activation='softmax'))\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","\n","def objective(trial):\n","    # Define a pruning callback\n","    pruning_callback = TFKerasPruningCallback(trial, \"val_accuracy\")\n","\n","    # Parameters to optimize\n","    conv_1_units = trial.suggest_int('conv_1_units', 4, 32, step=4)\n","    filter1 = trial.suggest_int('filter1', 3, 5, step=2)\n","    conv_2_units = trial.suggest_int('conv_2_units', 4, 32, step=4)\n","    filter2 = trial.suggest_int('filter2', 3, 5, step=2)\n","    fc_1_units = trial.suggest_int('fc_1_units', 10, 100, step=10)\n","\n","    # Build and compile the model with the suggested parameters\n","    model = build_model(conv_1_units, filter1, conv_2_units, filter2, fc_1_units)\n","\n","    # Train the model\n","    history = model.fit(X_train, y_train_ohe, epochs=epochs, batch_size=32,\n","                        validation_data=(X_val, y_val_ohe),\n","                        callbacks=[pruning_callback], verbose=0)\n","\n","    # Evaluate the model on the validation set\n","    val_acc = history.history['val_accuracy'][-1]\n","\n","    return val_acc\n","\n","\n","def test_optuna_n_times(feature='photo_path', epochs=epochs, n=nb_runs, norm=True, model_name='V4_optuna'):\n","    results_val_acc, results_time_fit,  results_time_predict = [], [], []\n","\n","    for i in range(n):\n","        with mlflow.start_run(run_name=f'run_{i}', nested=True) as kid:\n","            print(f'\\nRun {i}\\n')\n","            print(\"RUN ID \", kid.info.run_id)\n","\n","            # feature, target\n","            X_feature = feature_to_array(feature, norm=norm)\n","            y_target = np.asarray(data[\"target\"])\n","\n","            X_train_val, X_test, y_train_val, y_test = train_test_split(X_feature, y_target, test_size=0.1,\n","                                                                        shuffle=True, random_state=alea,\n","                                                                        stratify=y_target)\n","\n","            X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1,\n","                                                                        shuffle=True, random_state=alea,\n","                                                                        stratify=y_train_val)\n","\n","            y_train_ohe = tf.keras.utils.to_categorical(y_train)\n","            y_val_ohe = tf.keras.utils.to_categorical(y_val)\n","            y_test_ohe = tf.keras.utils.to_categorical(y_test)\n","\n","            study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n","            study.optimize(objective, n_trials=10)\n","\n","            # plot_optimization_history(study)\n","\n","            # Fetch the best parameters\n","            best_trial = study.best_trial\n","            best_params = best_trial.params\n","\n","            # Build the model with the best parameters\n","            model = build_model(**best_params)\n","\n","            # Fit model and time it\n","            time_fit_start = time.time()\n","            model.fit(X_train, y_train_ohe, epochs=epochs, batch_size=32,\n","                    validation_data=(X_val, y_val_ohe), verbose=0)\n","            time_fit_end = time.time()\n","            time_fit = time_fit_end - time_fit_start\n","\n","            # Time predictions\n","            time_predict_start = time.time()\n","            _, val_acc = model.evaluate(X_val, y_val_ohe)\n","            time_predict_end = time.time()\n","            time_predict = time_predict_end - time_predict_start\n","\n","            # print(f'Val accuracy (feature={feature}): {val_acc}')\n","\n","            parameters.update(best_params)\n","            mlflow.log_params(parameters)\n","\n","            save_model_if_better_than_the_best(model=model, acc=val_acc, best_params=parameters)\n","\n","            metriq = {\n","                \"accuracy_val\": val_acc,\n","                \"time_fit\": time_fit,\n","                \"time_predict\": time_predict\n","            }\n","            mlflow.log_metrics(metriq)\n","\n","            results_val_acc.append(val_acc)\n","            results_time_fit.append(time_fit)\n","            results_time_predict.append(time_predict)\n","\n","    # Calculate averages\n","    mean_val_acc = np.mean(results_val_acc)\n","    mean_time_fit = np.mean(results_time_fit)\n","    mean_time_predict = np.mean(results_time_predict)\n","\n","    # Calculate standard deviations\n","    std_val_acc = np.std(results_val_acc)\n","\n","    metrix = {\n","        \"accuracy_val_moy\": mean_val_acc,\n","        \"accuracy_val_std\": std_val_acc,\n","        \"time_fit_moy\": mean_time_fit,\n","        \"time_predict_moy\": mean_time_predict\n","    }\n","    for run in range(nb_runs):\n","        metrix[f'accuracy_val_run_{run}'] = results_val_acc[run]\n","\n","    # multiple metrics\n","    mlflow.log_metrics(metrix)\n","\n","    results = {'model': model_name,\n","               'df': 'data_3_classes',\n","               'feature': feature,\n","               'accuracy_val_moy': mean_val_acc,\n","               'accuracy_val_std': std_val_acc,\n","               'time_fit_moy (s)': mean_time_fit,\n","               'time_predict_moy (s)': mean_time_predict,\n","              }\n","\n","    # Append a new row for this model\n","    model_results.append(results)\n","\n","    print('\\n', f'Accuracy val {feature}, moyenne : {mean_val_acc}')\n","    print(f'Accuracy val {feature}, écart-type : {std_val_acc}')\n","\n","    return metrix\n","\n","\n","# _ = test_optuna_n_times()\n"]},{"cell_type":"code","execution_count":null,"id":"c0b1bde1","metadata":{},"outputs":[],"source":["with mlflow.start_run(run_name=\"optuna_1\", experiment_id=experiment_id) as parent:\n","    parameters = {\n","        \"model\": \"V4_optuna\",\n","        \"preprocessing\": 'norm',\n","        \"feature\": 'originale, photo_path',\n","        'target': f'{nb_classes}_classes',\n","        'dim images': f'{size_wh} x {size_wh} x 3',\n","        'size val test sets': test_size,\n","        'nb epochs': epochs\n","    }\n","    mlflow.log_params(parameters)\n","\n","    all_metrics = {}\n","\n","    metrix = test_optuna_n_times()\n","\n","    all_metrics[f'accuracy_val_moy_{feature}'] = metrix['accuracy_val_moy']\n","\n","    print('\\n')\n","    pprint(all_metrics)\n","    mlflow.log_metrics(all_metrics)\n"]},{"cell_type":"code","execution_count":null,"id":"bee2597b","metadata":{},"outputs":[],"source":["# {'conv_1_units': 28, 'filter1': 3, 'conv_2_units': 4, 'filter2': 5, 'fc_1_units': 28}. Best is trial 8 with value: 0.6451612710952759.\n","# value: 0.6666666865348816 and parameters: {'conv_1_units': 8, 'filter1': 5, 'conv_2_units': 12, 'filter2': 5, 'fc_1_units': 80}\n","\n","affichage_results(best_acc=best_acc)\n"]},{"cell_type":"markdown","id":"aeb1e92b","metadata":{},"source":["### optimizer\n"]},{"cell_type":"code","execution_count":null,"id":"03413e5f","metadata":{},"outputs":[],"source":["experiment_id = create_mlflow_experiment(\n","    experiment_name='optuna_2_optimizer',\n","    artifact_location=\"./mlruns/artifacts\",\n","    tags={\"model\": \"V4\", \"preprocessing\": 'norm', \"feature\": \"photo_path\", 'target': '3_classes'},\n",")\n","\n","experiment = get_mlflow_experiment(experiment_id=experiment_id)\n","print(\"Name: {}\".format(experiment.name))\n"]},{"cell_type":"code","execution_count":null,"id":"54c39dd5","metadata":{},"outputs":[],"source":["#\n","\n","def build_model(conv_1_units, filter1, conv_2_units, filter2, fc_1_units, optimizer):\n","    model = Sequential()\n","    model.add(Conv2D(conv_1_units, kernel_size=(filter1, filter1),\n","                     activation=\"tanh\",\n","                     input_shape=(size_wh, size_wh, 3)))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Conv2D(conv_2_units, kernel_size=(filter2, filter2),\n","                     activation=\"tanh\"))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(fc_1_units, activation='tanh'))\n","    model.add(Dense(nb_classes, activation='softmax'))\n","\n","    # Compile the model\n","    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","\n","def objective(trial):\n","    # Define a pruning callback\n","    pruning_callback = TFKerasPruningCallback(trial, \"val_accuracy\")\n","\n","    # Parameters to optimize\n","    conv_1_units = trial.suggest_int('conv_1_units', 4, 32, step=4)\n","    filter1 = trial.suggest_int('filter1', 3, 5, step=2)\n","    conv_2_units = trial.suggest_int('conv_2_units', 4, 32, step=4)\n","    filter2 = trial.suggest_int('filter2', 3, 5, step=2)\n","    fc_1_units = trial.suggest_int('fc_1_units', 10, 100, step=10)\n","    optimizer = trial.suggest_categorical('optimizer', [\"Adam\", \"RMSprop\", \"SGD\"])\n","\n","    # Build and compile the model with the suggested parameters\n","    model = build_model(conv_1_units, filter1, conv_2_units, filter2, fc_1_units, optimizer)\n","\n","    # Train the model\n","    history = model.fit(X_train, y_train_ohe, epochs=epochs, batch_size=32,\n","                        validation_data=(X_val, y_val_ohe),\n","                        callbacks=[pruning_callback], verbose=0)\n","\n","    # Evaluate the model on the validation set\n","    val_acc = history.history['val_accuracy'][-1]\n","\n","    return val_acc\n"]},{"cell_type":"code","execution_count":null,"id":"3387abc9","metadata":{},"outputs":[],"source":["with mlflow.start_run(run_name=\"optuna_2\", experiment_id=experiment_id) as parent:\n","    parameters = {\n","        \"model\": \"V5_optuna\",\n","        \"preprocessing\": 'norm',\n","        \"feature\": 'originale, photo_path',\n","        'target': f'{nb_classes}_classes',\n","        'dim images': f'{size_wh} x {size_wh} x 3',\n","        'size val test sets': test_size,\n","        'nb epochs': epochs\n","    }\n","    mlflow.log_params(parameters)\n","\n","    all_metrics = {}\n","\n","    metrix = test_optuna_n_times(model_name='V5_optuna')\n","\n","    all_metrics[f'accuracy_val_moy_{feature}'] = metrix['accuracy_val_moy']\n","\n","    print('\\n')\n","    pprint(all_metrics)\n","    mlflow.log_metrics(all_metrics)\n"]},{"cell_type":"code","execution_count":null,"id":"06f2420d","metadata":{},"outputs":[],"source":["# 0.6236559152603149 and parameters: {'conv_1_units': 20, 'filter1': 3, 'conv_2_units': 4, 'filter2': 5, 'fc_1_units': 10, 'optimizer': 'Adam'}\n","# 0.6129032373428345 and parameters: {'conv_1_units': 8, 'filter1': 3, 'conv_2_units': 8, 'filter2': 5, 'fc_1_units': 20, 'optimizer': 'Adam'}. Best is trial 7 with value: 0.6129032373428345.\n","\n","affichage_results(best_acc=best_acc)\n"]},{"cell_type":"code","execution_count":null,"id":"3753c0dc","metadata":{},"outputs":[],"source":["# gridsize smaller grids ?\n","\n","# Optimizer: The Adam optimizer is a good choice, but you might also experiment with other optimizers\n","# such as RMSprop or SGD (Stochastic Gradient Descent) with momentum.\n","\n","# Loss Function: Cross-entropy loss is appropriate for classification tasks with softmax activation\n","# in the output layer, so categorical_crossentropy is fine.\n","\n","# Model Capacity: LeNet is a relatively shallow network compared to modern architectures.\n","# Depending on the complexity of your dataset, you might need to adjust the model's capacity\n","# by adding more convolutional layers or increasing the number of units in the fully connected layers.\n","\n","# Regularization: You may consider adding regularization techniques such as dropout or weight decay\n","# to prevent overfitting, especially if you observe overfitting during training.\n","\n","# modify input shape ?\n","\n","# inception module + global average pooling ?\n"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
